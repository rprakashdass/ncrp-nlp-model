{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cddcccc1f1cfacd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocessing the dataset",
   "id": "5e666dee575729da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ],
   "id": "e23dac7f3dc70afe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"train.csv\", encoding=\"ISO-8859-1\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "data = data.rename(columns={'crimeaditionalinfo': 'text'})\n",
    "data = data.dropna(subset=['text'])\n",
    "test = test.rename(columns={'crimeaditionalinfo': 'text'}) # renaming the crime info column\n",
    "test = test.dropna(subset=['text']) # dropping all entries with no information on the crime"
   ],
   "id": "ed73f2c8624cfeb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "null_count = data['sub_category'].isnull().sum()\n",
    "data['sub_category'].fillna(data['category'], inplace=True)\n",
    "test['sub_category'].fillna(data['category'], inplace=True) # replacing the null entries in sub_category column with the category of the complaint\n",
    "print('null count: ', null_count)\n",
    "data.groupby('category')['sub_category'].value_counts() # number of entries and sub categories under each categories"
   ],
   "id": "edf05ea6b2334974"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mapping = data.set_index('sub_category')['category'].to_dict()\n",
    "def get_category(sub_category):\n",
    "    return mapping.get(sub_category)\n",
    "subcategories = data['sub_category'].unique().tolist()\n"
   ],
   "id": "a26b406754b2feef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unique_data_subcategories = set(data['sub_category'].unique())\n",
    "unique_test_subcategories = set(test['sub_category'].unique())\n",
    "\n",
    "exclusive_in_data_subcategories = unique_data_subcategories - unique_test_subcategories\n",
    "exclusive_in_test_subcategories = unique_test_subcategories - unique_data_subcategories\n",
    "\n",
    "print(\"Subcategories exclusive to 'train' dataframe:\")\n",
    "print(list(exclusive_in_data_subcategories))\n",
    "print(\"\\nSubcategories exclusive to 'test' dataframe:\")\n",
    "print(list(exclusive_in_test_subcategories))\n",
    "exclusive_in_test_subcategories"
   ],
   "id": "b63baa96b54f6b3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unique_data_categories = set(data['category'].unique())\n",
    "unique_test_categories = set(test['category'].unique())\n",
    "\n",
    "exclusive_in_data = unique_data_categories - unique_test_categories\n",
    "exclusive_in_test = unique_test_categories - unique_data_categories\n",
    "\n",
    "exclusive_in_data, exclusive_in_test\n"
   ],
   "id": "2554d43d83d74d0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = data[data['word_count'] >= 4]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['text'] = data['text'].str.lower().apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
    "test['text'] = test['text'].str.lower().apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))"
   ],
   "id": "e06d2bac4d00b092"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Removing all instances which are not of type string\n",
    "data = data[data['text'].apply(lambda x: isinstance(x, str))]\n",
    "test = test[test['text'].apply(lambda x: isinstance(x, str))]\n",
    "total_char_count = data['text'].str.len().sum()\n",
    "total_char_count"
   ],
   "id": "dd1268e5fa11e561"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def Text_Cleaning(Text):\n",
    "  Text = Text.lower()\n",
    "  punc = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "  Text = Text.translate(punc)\n",
    "  Text = re.sub(r'\\d+', '', Text)\n",
    "  Text = re.sub('https?://\\S+|www\\.\\S+', '', Text)\n",
    "  Text = re.sub('\\n', '', Text)\n",
    "  return Text\n",
    "Stopwords = set(nltk.corpus.stopwords.words(\"english\")) - set([\"not\"])\n",
    "\n",
    "def Text_Processing(Text):\n",
    "    Processed_Text = list()\n",
    "    Lemmatizer = WordNetLemmatizer()\n",
    "    Tokens = nltk.word_tokenize(Text)\n",
    "\n",
    "    for word in Tokens:\n",
    "        if word not in Stopwords:\n",
    "            Processed_Text.append(Lemmatizer.lemmatize(word))\n",
    "\n",
    "    return \" \".join(Processed_Text)"
   ],
   "id": "334a586fc28fc19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data[\"text\"] = data[\"text\"].apply(lambda Text: Text_Cleaning(Text))\n",
    "test[\"text\"] = test[\"text\"].apply(lambda Text: Text_Cleaning(Text))\n",
    "data[\"text\"] = data[\"text\"].apply(lambda Text: Text_Processing(Text))\n",
    "test[\"text\"] = test[\"text\"].apply(lambda Text: Text_Processing(Text))\n",
    "total_char_count = data['text'].str.len().sum()\n",
    "total_char_count"
   ],
   "id": "2816f4cfebf22202"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stemmer = PorterStemmer()\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(x)]))\n",
    "test['text'] = test['text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(x)]))"
   ],
   "id": "97081dfb0a411846"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data.to_csv('cleaned_train.csv', index=False)\n",
    "test.to_csv('cleaned_test.csv', index=False)"
   ],
   "id": "d3f1778d15f311aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Logistic Regression",
   "id": "7ac2474e3655614f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T04:57:20.739986Z",
     "start_time": "2024-11-22T04:56:49.091980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('cleaned_train.csv')\n",
    "df_test = pd.read_csv('cleaned_test.csv')\n",
    "df_train['text'] = df_train['text'].astype(str).fillna(\"\")\n",
    "df_test['text'] = df_test['text'].astype(str).fillna(\"\")\n",
    "\n",
    "\n",
    "# Encoding categories and sub-categories\n",
    "category_encoder = LabelEncoder()\n",
    "sub_category_encoder = LabelEncoder()\n",
    "\n",
    "# Custom function for safe encoding\n",
    "def safe_transform(encoder, data):\n",
    "    # Get the unique classes seen by the encoder\n",
    "    classes = set(encoder.classes_)\n",
    "\n",
    "    # Replace unseen labels with -1\n",
    "    return [encoder.transform([label])[0] if label in classes else -1 for label in data]\n",
    "\n",
    "category_encoder.fit_transform(df_train['category'])\n",
    "\n",
    "# Transform the train data\n",
    "df_train['category_label'] = category_encoder.transform(df_train['category'])\n",
    "\n",
    "# Transform the test data (with safe handling for unseen labels)\n",
    "df_test['category_label'] = safe_transform(category_encoder, df_test['category'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed\n",
    "\n",
    "# Fit on training text data and transform both train and test\n",
    "X_train_tfidf = vectorizer.fit_transform(df_train['text'])\n",
    "X_test_tfidf = vectorizer.transform(df_test['text'])\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train_category, y_test_category = X_train_tfidf, X_test_tfidf, df_train['category_label'], df_test['category_label']\n",
    "\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "category_model = LogisticRegression(max_iter=1000)  # Increase `max_iter` if convergence issues occur\n",
    "\n",
    "# Train the Logistic Regression model on the category labels\n",
    "category_model.fit(X_train, y_train_category)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_category = category_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_category, y_pred_category)\n",
    "print(f\"Category Model Accuracy (Logistic Regression): {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "prediction = category_model.predict(X_test_tfidf[0])\n",
    "print(df_test['text'][0])\n",
    "print(category_encoder.inverse_transform(prediction))\n",
    "\n",
    "\n",
    "sub_category_models = {}\n",
    "\n",
    "category_groups = df_train.groupby('category_label')\n",
    "\n",
    "for category, group in category_groups:\n",
    "    # Extract features and labels for this category\n",
    "    X_category = X_train_tfidf[group.index]\n",
    "    y_sub_category = group['sub_category']\n",
    "\n",
    "    # Initialize and fit a LabelEncoder\n",
    "    sub_category_encoder = LabelEncoder()\n",
    "    y_sub_category_encoded = sub_category_encoder.fit_transform(y_sub_category)\n",
    "\n",
    "    # Check if there's only one unique sub-category\n",
    "    if len(set(y_sub_category_encoded)) == 1:\n",
    "        # Save the constant prediction (always the single label)\n",
    "        sub_category_models[category] = {\n",
    "            \"model\": None,\n",
    "            \"encoder\": sub_category_encoder,\n",
    "            \"constant_label\": y_sub_category.iloc[0]\n",
    "        }\n",
    "        continue\n",
    "\n",
    "    # Train a Logistic Regression model for sub-category\n",
    "    sub_category_model = LogisticRegression(max_iter=1000)\n",
    "    sub_category_model.fit(X_category, y_sub_category_encoded)\n",
    "\n",
    "    # Save the trained model and encoder in the dictionary\n",
    "    sub_category_models[category] = {\n",
    "        \"model\": sub_category_model,\n",
    "        \"encoder\": sub_category_encoder\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_single_sample(X_sample):\n",
    "    # Ensure the input sample is reshaped correctly for prediction\n",
    "    if len(X_sample.shape) == 1:\n",
    "        X_sample = X_sample.reshape(1, -1)\n",
    "\n",
    "    # Step 1: Predict the category\n",
    "    predicted_category = category_model.predict(X_sample)[0]\n",
    "\n",
    "    # Step 2: Retrieve the sub-category model and encoder for the predicted category\n",
    "    sub_category_data = sub_category_models.get(predicted_category)\n",
    "\n",
    "    if sub_category_data is None:\n",
    "        raise ValueError(f\"No sub-category model found for category {predicted_category}.\")\n",
    "\n",
    "    sub_category_model = sub_category_data.get('model')  # Retrieve the model\n",
    "    sub_category_encoder = sub_category_data.get('encoder')  # Retrieve the encoder\n",
    "\n",
    "    # Step 3: Handle single sub-category case (model not trained)\n",
    "    if sub_category_model is None:\n",
    "        # Directly decode the only possible sub-category\n",
    "        single_sub_category = sub_category_encoder.inverse_transform([0])[0]\n",
    "        return predicted_category, single_sub_category\n",
    "\n",
    "    # Step 4: Predict the sub-category using the trained model\n",
    "    predicted_sub_category_encoded = sub_category_model.predict(X_sample)[0]\n",
    "    predicted_sub_category = sub_category_encoder.inverse_transform([predicted_sub_category_encoded])[0]\n",
    "\n",
    "    return predicted_category, predicted_sub_category\n",
    "\n",
    "\n",
    "def evaluate_combined_model(X_test_tfidf, y_test_category, df_test):\n",
    "    category_accuracy = 0\n",
    "    combined_accuracy = 0\n",
    "\n",
    "    # Convert X_test_tfidf to dense format for prediction\n",
    "    X_test_dense = X_test_tfidf.toarray()\n",
    "    total_samples = len(X_test_dense)\n",
    "    # Track correct predictions\n",
    "    correct_category_predictions = 0\n",
    "    correct_combined_predictions = 0\n",
    "\n",
    "    for i in range(total_samples):\n",
    "        # Get the true category and sub-category for the current sample\n",
    "        true_category = y_test_category.iloc[i]\n",
    "        true_sub_category = df_test.iloc[i]['sub_category']\n",
    "\n",
    "        # Step 1: Predict the category\n",
    "        predicted_category = category_model.predict([X_test_dense[i]])[0]\n",
    "\n",
    "        # Check if the category prediction is correct\n",
    "        if predicted_category == true_category:\n",
    "            correct_category_predictions += 1\n",
    "\n",
    "            # Step 2: Predict the sub-category for the correct category\n",
    "            sub_category_model_info = sub_category_models.get(true_category, None)\n",
    "            if sub_category_model_info:\n",
    "                sub_category_model = sub_category_model_info.get(\"model\", None)\n",
    "                if sub_category_model:\n",
    "                    # Predict sub-category using the model\n",
    "                    predicted_sub_category_encoded = sub_category_model.predict([X_test_dense[i]])[0]\n",
    "                    predicted_sub_category = sub_category_model_info['encoder'].inverse_transform([predicted_sub_category_encoded])[0]\n",
    "                else:\n",
    "                    # No model (only one sub-category), use the constant label\n",
    "                    predicted_sub_category = sub_category_model_info['constant_label']\n",
    "            else:\n",
    "                # No model (missing sub-category model), predict None or fallback\n",
    "                predicted_sub_category = None\n",
    "\n",
    "            # Check if sub-category prediction is correct\n",
    "            if predicted_sub_category == true_sub_category:\n",
    "                correct_combined_predictions += 1\n",
    "\n",
    "    # Calculate accuracies\n",
    "    category_accuracy = correct_category_predictions / total_samples\n",
    "    combined_accuracy = correct_combined_predictions / total_samples\n",
    "\n",
    "    return {\n",
    "        \"category_accuracy\": category_accuracy,\n",
    "        \"combined_accuracy\": combined_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Now, let's evaluate the model:\n",
    "results = evaluate_combined_model(X_test_tfidf, y_test_category, df_test)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Category Accuracy: {results['category_accuracy']:.2f}\")\n",
    "print(f\"Combined Accuracy: {results['combined_accuracy']:.2f}\")\n",
    "\n",
    "# Category Classification Report\n",
    "# True labels and predictions for categories\n",
    "y_true_category = df_test['category_label']\n",
    "y_pred_category = category_model.predict(X_test_tfidf)\n",
    "\n",
    "# Generate classification report for categories\n",
    "category_report = classification_report(\n",
    "    y_true_category,\n",
    "    y_pred_category,\n",
    "    target_names=category_encoder.classes_,\n",
    "    zero_division=0\n",
    ")\n",
    "print(\"Category Classification Report:\\n\")\n",
    "print(category_report)\n",
    "\n",
    "# Sub-Category Classification Report\n",
    "# Initialize variables to store true and predicted sub-categories\n",
    "y_true_sub_category = []\n",
    "y_pred_sub_category = []\n",
    "\n",
    "# Convert X_test_tfidf to dense format for prediction\n",
    "X_test_dense = X_test_tfidf.toarray()\n",
    "\n",
    "# Iterate through each test sample\n",
    "for i in range(len(X_test_dense)):\n",
    "    true_category = y_true_category.iloc[i]\n",
    "    true_sub_category = df_test.iloc[i]['sub_category']\n",
    "\n",
    "    # Predict the category first\n",
    "    predicted_category = category_model.predict([X_test_dense[i]])[0]\n",
    "\n",
    "    # Retrieve the sub-category model and encoder for the predicted category\n",
    "    sub_category_model_info = sub_category_models.get(predicted_category, None)\n",
    "    if sub_category_model_info:\n",
    "        sub_category_model = sub_category_model_info.get(\"model\", None)\n",
    "        sub_category_encoder = sub_category_model_info.get(\"encoder\", None)\n",
    "\n",
    "        if sub_category_model:\n",
    "            # Predict sub-category using the model\n",
    "            predicted_sub_category_encoded = sub_category_model.predict([X_test_dense[i]])[0]\n",
    "            predicted_sub_category = sub_category_encoder.inverse_transform([predicted_sub_category_encoded])[0]\n",
    "        else:\n",
    "            # Use constant label if no model is available\n",
    "            predicted_sub_category = sub_category_model_info['constant_label']\n",
    "    else:\n",
    "        predicted_sub_category = None\n",
    "\n",
    "    # Append true and predicted sub-categories for the classification report\n",
    "    y_true_sub_category.append(true_sub_category)\n",
    "    y_pred_sub_category.append(predicted_sub_category)\n",
    "\n",
    "# Generate classification report for sub-categories\n",
    "sub_category_report = classification_report(\n",
    "    y_true_sub_category,\n",
    "    y_pred_sub_category,\n",
    "    zero_division=0\n",
    ")\n",
    "print(\"\\nSub-Category Classification Report:\\n\")\n",
    "print(sub_category_report)"
   ],
   "id": "e48623543a8bdfd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Model Accuracy (Logistic Regression): 0.76\n",
      "sir namaskar mein ranjit kumar patrapais nehi tho sir kuch din pehel onlin loan aap credit pearl loan aap se money loan kiya thalekin sir loan bolk jub loan diy tho mein turant return kar diya thalekin din baad what app pe messag aya payment karomein bola diy aap mein wo de diyawo gali diy tho v return kar diyafir v messag kark bolt hai full payment karo half payment nehi chalegarap case mein daldeng etcfak illig se contact number v hack kar dete haibol rahehai sab ko messag kareng ye rapist hai bolk sirpl sir small ammount ke liy goggl play store se loan appli kiya thafak loan aap v hai socha nehi thapl sir request kar rahahun action lo sir mera number hai jo v proof chahiy dunga sir\n",
      "['Online Financial Fraud']\n",
      "Category Accuracy: 0.76\n",
      "Combined Accuracy: 0.51\n",
      "Category Classification Report:\n",
      "\n",
      "                                                      precision    recall  f1-score   support\n",
      "\n",
      "                               Any Other Cyber Crime       0.00      0.00      0.00         4\n",
      "Child Pornography CPChild Sexual Abuse Material CSAM       0.44      0.24      0.31      3670\n",
      "                                Cryptocurrency Crime       0.70      0.26      0.38       123\n",
      "                      Cyber Attack/ Dependent Crimes       0.66      0.46      0.54       166\n",
      "                                     Cyber Terrorism       1.00      1.00      1.00      1261\n",
      "      Hacking  Damage to computercomputer system etc       0.00      0.00      0.00        52\n",
      "                            Online Cyber Trafficking       0.42      0.22      0.29       592\n",
      "                              Online Financial Fraud       0.00      0.00      0.00        61\n",
      "                            Online Gambling  Betting       0.81      0.95      0.87     18890\n",
      "               Online and Social Media Related Crime       0.75      0.04      0.08       134\n",
      "                                          Ransomware       0.56      0.59      0.57      4139\n",
      "           RapeGang Rape RGRSexually Abusive Content       0.50      0.06      0.10        18\n",
      "                             Report Unlawful Content       1.00      0.91      0.95       912\n",
      "                               Sexually Explicit Act       0.28      0.03      0.05       535\n",
      "                           Sexually Obscene material       0.47      0.12      0.19       665\n",
      "\n",
      "                                            accuracy                           0.76     31222\n",
      "                                           macro avg       0.50      0.32      0.36     31222\n",
      "                                        weighted avg       0.72      0.76      0.72     31222\n",
      "\n",
      "\n",
      "Sub-Category Classification Report:\n",
      "\n",
      "                                                                      precision    recall  f1-score   support\n",
      "\n",
      "                                               Any Other Cyber Crime       0.00      0.00      0.00       261\n",
      "                             Business Email CompromiseEmail Takeover       0.33      0.04      0.08        90\n",
      "                                           Cheating by Impersonation       0.19      0.06      0.09       719\n",
      "                Child Pornography CPChild Sexual Abuse Material CSAM       0.00      0.00      0.00        10\n",
      "                                        Computer Generated CSAM/CSEM       0.00      0.00      0.00         2\n",
      "                                                Cryptocurrency Crime       0.00      0.00      0.00        14\n",
      "                                                Cryptocurrency Fraud       0.66      0.46      0.54       166\n",
      "                                      Cyber Attack/ Dependent Crimes       0.00      0.00      0.00        97\n",
      "                                    Cyber Blackmailing & Threatening       0.00      0.00      0.00         1\n",
      "                                   Cyber Bullying  Stalking  Sexting       0.39      0.68      0.50      1366\n",
      "                                                     Cyber Terrorism       0.00      0.00      0.00        56\n",
      "                             Damage to computer computer systems etc       0.33      0.03      0.05        39\n",
      "                                                   Data Breach/Theft       0.16      0.05      0.08       171\n",
      "                                DebitCredit Card FraudSim Swap Fraud       0.68      0.68      0.68      3555\n",
      "                                               DematDepository Fraud       0.26      0.04      0.06       222\n",
      "Denial of Service (DoS)/Distributed Denial of Service (DDOS) attacks       0.16      0.10      0.12       187\n",
      "                                                      EMail Phishing       0.33      0.06      0.10        54\n",
      "                                               EWallet Related Fraud       0.63      0.40      0.49      1338\n",
      "                                                       Email Hacking       0.48      0.29      0.36       130\n",
      "                                           FakeImpersonating Profile       0.42      0.42      0.42       763\n",
      "                                                   Fraud CallVishing       0.25      0.31      0.28      1826\n",
      "                      Hacking  Damage to computercomputer system etc       0.00      0.00      0.00        37\n",
      "                                                  Hacking/Defacement       0.15      0.26      0.19       200\n",
      "                                                 Impersonating Email       0.00      0.00      0.00        13\n",
      "                                      Internet Banking Related Fraud       0.68      0.55      0.61      2973\n",
      "                                                  Intimidating Email       0.00      0.00      0.00        11\n",
      "                                                      Malware Attack       0.12      0.11      0.11       170\n",
      "                                            Online Cyber Trafficking       0.00      0.00      0.00         4\n",
      "                                              Online Financial Fraud       0.00      0.00      0.00      1381\n",
      "                                            Online Gambling  Betting       0.75      0.04      0.08       147\n",
      "                                                    Online Job Fraud       0.34      0.17      0.23       294\n",
      "                                            Online Matrimonial Fraud       0.00      0.00      0.00        38\n",
      "                                                  Online Trafficking       0.00      0.00      0.00        61\n",
      "                               Online and Social Media Related Crime       0.00      0.00      0.00       268\n",
      "                                                               Other       0.44      0.24      0.31      3670\n",
      "                                      Profile Hacking Identity Theft       0.44      0.44      0.44       751\n",
      "                                Provocative Speech for unlawful acts       0.41      0.11      0.17       130\n",
      "                                                          Ransomware       0.50      0.05      0.10        19\n",
      "                                                   Ransomware Attack       0.13      0.19      0.16       186\n",
      "                           RapeGang Rape RGRSexually Abusive Content       0.03      0.32      0.05        66\n",
      "                                                       SQL Injection       0.12      0.09      0.10       167\n",
      "                                                   Sexual Harassment       0.00      0.00      0.00         1\n",
      "                                               Sexually Explicit Act       0.00      0.00      0.00        32\n",
      "                                           Sexually Obscene material       0.02      0.06      0.03        47\n",
      "                            Tampering with computer source documents       0.10      0.11      0.10       194\n",
      "                                                  UPI Related Frauds       0.59      0.87      0.70      8886\n",
      "                                      Unauthorised AccessData Breach       0.34      0.21      0.26       370\n",
      "                                           Website DefacementHacking       0.00      0.00      0.00        39\n",
      "\n",
      "                                                            accuracy                           0.51     31222\n",
      "                                                           macro avg       0.22      0.16      0.16     31222\n",
      "                                                        weighted avg       0.47      0.51      0.47     31222\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LSTM\n",
   "id": "7b846bc3da1f9a44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T05:52:20.575686Z",
     "start_time": "2024-11-22T05:43:41.065783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "data = pd.read_csv('cleaned_train.csv')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "max_words = 10000  # Limit the number of unique words to 10,000\n",
    "max_sequence_length = 100\n",
    "\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "X = tokenizer.texts_to_sequences(data['text'])\n",
    "X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "# Label Encoding the categories (You can also encode subcategories in a similar way if needed)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['category'])\n",
    "y = to_categorical(y)  # Convert labels to one-hot encoding\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Build the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))  # Output layer for multi-class classification\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "# Save the model\n",
    "model.save('lstm_model.h5')\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the Pre-Trained Model\n",
    "model = load_model('lstm_model.h5')  # Load your pre-trained model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 2. Load the Dataset (cleaned_test.csv)\n",
    "df = pd.read_csv(\"cleaned_test.csv\")\n",
    "\n",
    "# Ensure the 'text' column contains strings\n",
    "df['text'] = df['text'].astype(str).fillna(\"\")\n",
    "X_test_data = df['text']\n",
    "\n",
    "# Ensure the 'category' column has no NaN values\n",
    "df['category'] = df['category'].fillna(\"Unknown\")\n",
    "y_test = df['category']\n",
    "\n",
    "# 3. Tokenization and Padding\n",
    "# Initialize the Tokenizer (same parameters as during training)\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_test_data)  # Use the same tokenizer as during training\n",
    "\n",
    "# Tokenize and pad test data\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_data)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "# 4. Predict Using the Pre-Trained Model\n",
    "y_pred_prob = model.predict(X_test_padded)  # Get predicted probabilities\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# 5. Encode the True Labels (if LabelEncoder was used during training)\n",
    "label_encoder = LabelEncoder()\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)  # Encode the true labels\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"absl\")\n",
    "# 6. Calculate the Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "precision = precision_score(y_test_encoded, y_pred, average='weighted')\n",
    "recall = recall_score(y_test_encoded, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test_encoded, y_pred, average='weighted')\n",
    "\n",
    "# 7. Print the Metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ],
   "id": "e6492348225c0528",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pradeesh11/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pradeesh11/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m1096/1096\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m97s\u001B[0m 88ms/step - accuracy: 0.6885 - loss: 1.0531 - val_accuracy: 0.7430 - val_loss: 0.7732\n",
      "Epoch 2/5\n",
      "\u001B[1m1096/1096\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m98s\u001B[0m 89ms/step - accuracy: 0.7478 - loss: 0.7593 - val_accuracy: 0.7466 - val_loss: 0.7431\n",
      "Epoch 3/5\n",
      "\u001B[1m1096/1096\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m97s\u001B[0m 89ms/step - accuracy: 0.7631 - loss: 0.6993 - val_accuracy: 0.7481 - val_loss: 0.7259\n",
      "Epoch 4/5\n",
      "\u001B[1m1096/1096\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m98s\u001B[0m 89ms/step - accuracy: 0.7793 - loss: 0.6368 - val_accuracy: 0.7523 - val_loss: 0.7259\n",
      "Epoch 5/5\n",
      "\u001B[1m1096/1096\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m100s\u001B[0m 91ms/step - accuracy: 0.7961 - loss: 0.5901 - val_accuracy: 0.7504 - val_loss: 0.7555\n",
      "\u001B[1m548/548\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 16ms/step - accuracy: 0.7470 - loss: 0.7707\n",
      "Test Accuracy: 75.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m976/976\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 15ms/step\n",
      "Accuracy: 0.0031\n",
      "Precision: 0.0288\n",
      "Recall: 0.0031\n",
      "F1 Score: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradeesh11/Documents/ncrp-nlp-model/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Random Forest Classifier",
   "id": "b58bac45dc6ebb28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T05:08:37.500133Z",
     "start_time": "2024-11-22T05:07:41.749754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"cleaned_train.csv\")\n",
    "df['text'] = df['text'].fillna('')\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the entire dataset\n",
    "X_vec = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Remove classes with only one sample\n",
    "counts = df['category'].value_counts()\n",
    "low_count_classes = counts[counts <= 1].index\n",
    "df_filtered = df[~df['category'].isin(low_count_classes)]\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'category' is the column with categories\n",
    "df = df_filtered['category'].value_counts()\n",
    "df = df_filtered\n",
    "\n",
    "\n",
    "df.to_csv(\"unique_categories.csv\", index=False)\n",
    "\n",
    "\n",
    "# Define your features and target\n",
    "X = df['text']\n",
    "y = df['category']  # Replace with actual category column\n",
    "\n",
    "# Split the data while keeping the indices\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# After splitting, we will also need to transform the text data\n",
    "vectorizer = TfidfVectorizer(max_features=8000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df['text']\n",
    "y = df['category']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_vectorized)\n",
    "\n",
    "\n",
    "\n",
    "# Print accuracy score\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "results_df = pd.DataFrame({\n",
    "    'cleaned_text': X_test,  # Use original text from X_test\n",
    "    'predicted_category': y_pred,\n",
    "    'actual_category': y_test\n",
    "})\n",
    "\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('main_category_classification_results.csv', index=False)\n",
    "\n",
    "# Save the Random Forest model\n",
    "# joblib.dump(rf_model_model, 'random_forest_model.pkl')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "# joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "\n",
    "# Load the saved Random Forest model\n",
    "rf_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Load the saved TF-IDF vectorizer\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Print classification report\n",
    "print(\"Main Category Classification Report:\\n\", classification_report(y_test, y_pred,zero_division=0))"
   ],
   "id": "37c11e4491281e0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7525396644218697\n",
      "Main Category Classification Report:\n",
      "                                                       precision    recall  f1-score   support\n",
      "\n",
      "                               Any Other Cyber Crime       0.66      0.09      0.16      2116\n",
      "Child Pornography CPChild Sexual Abuse Material CSAM       0.87      0.21      0.33        63\n",
      "                                Cryptocurrency Crime       0.89      0.07      0.13       113\n",
      "                      Cyber Attack/ Dependent Crimes       1.00      1.00      1.00       741\n",
      "                                     Cyber Terrorism       0.00      0.00      0.00        33\n",
      "      Hacking  Damage to computercomputer system etc       0.85      0.09      0.16       322\n",
      "                            Online Cyber Trafficking       0.00      0.00      0.00        31\n",
      "                              Online Financial Fraud       0.76      0.98      0.86     10440\n",
      "                            Online Gambling  Betting       0.00      0.00      0.00        77\n",
      "               Online and Social Media Related Crime       0.57      0.58      0.58      2327\n",
      "                                          Ransomware       0.00      0.00      0.00        13\n",
      "           RapeGang Rape RGRSexually Abusive Content       1.00      0.92      0.96       570\n",
      "                               Sexually Explicit Act       0.91      0.06      0.11       332\n",
      "                           Sexually Obscene material       0.75      0.08      0.14       344\n",
      "\n",
      "                                            accuracy                           0.75     17522\n",
      "                                           macro avg       0.59      0.29      0.32     17522\n",
      "                                        weighted avg       0.74      0.75      0.69     17522\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "XGBOOST",
   "id": "4e49b2c7f2311479"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T04:39:50.656400Z",
     "start_time": "2024-11-22T04:35:11.838881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "data = pd.read_csv('cleaned_train.csv')\n",
    "\n",
    "# Step 2: Preprocess the Data\n",
    "data['text'] = data['text'].astype(str).fillna(\"\")  # Ensure text data is clean\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['category_encoded'] = label_encoder.fit_transform(data['category'])\n",
    "\n",
    "# Step 3: Split the Data\n",
    "X = data['text']  # Input text\n",
    "y = data['category_encoded']  # Encoded labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Convert Text to Features Using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Step 5: Train the XGBoost Model with Progress Bar\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='mlogloss', objective='multi:softmax', num_class=len(label_encoder.classes_))\n",
    "\n",
    "# Wrap the training process with tqdm for the progress bar\n",
    "with tqdm(total=100, desc=\"Training Progress\", unit=\"step\") as pbar:\n",
    "    for _ in range(1):  # Dummy loop to simulate steps; XGBoost handles training internally\n",
    "        xgb_model.fit(\n",
    "            X_train_tfidf,\n",
    "            y_train,\n",
    "            eval_set=[(X_test_tfidf, y_test)],\n",
    "            verbose=False\n",
    "        )\n",
    "        pbar.update(100)  # Update progress bar to 100% since training is one complete step\n",
    "\n",
    "# Step 6: Predict on the Test Data\n",
    "y_pred = xgb_model.predict(X_test_tfidf)\n",
    "unique_classes = sorted(set(y_test))  # Ensure all classes in y_test are covered\n",
    "class_names = label_encoder.inverse_transform(unique_classes)\n",
    "# Step 7: Generate the Classification Report\n",
    "report = classification_report(y_test, y_pred,\n",
    "                               labels=unique_classes,\n",
    "                               target_names=class_names,\n",
    "                               zero_division=0)\n",
    "print(report)\n"
   ],
   "id": "2b68f02846f35116",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 100/100 [04:33<00:00,  2.73s/step]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      precision    recall  f1-score   support\n",
      "\n",
      "                               Any Other Cyber Crime       0.48      0.23      0.31      2064\n",
      "Child Pornography CPChild Sexual Abuse Material CSAM       0.71      0.27      0.39        63\n",
      "                                Cryptocurrency Crime       0.67      0.39      0.49       102\n",
      "                      Cyber Attack/ Dependent Crimes       1.00      1.00      1.00       715\n",
      "                                     Cyber Terrorism       0.00      0.00      0.00        33\n",
      "      Hacking  Damage to computercomputer system etc       0.49      0.33      0.40       349\n",
      "                            Online Cyber Trafficking       0.00      0.00      0.00        29\n",
      "                              Online Financial Fraud       0.80      0.95      0.87     10497\n",
      "                            Online Gambling  Betting       0.50      0.05      0.09        85\n",
      "               Online and Social Media Related Crime       0.57      0.60      0.58      2339\n",
      "                                          Ransomware       0.00      0.00      0.00        13\n",
      "           RapeGang Rape RGRSexually Abusive Content       0.99      0.94      0.97       565\n",
      "                               Sexually Explicit Act       0.35      0.05      0.09       323\n",
      "                           Sexually Obscene material       0.30      0.08      0.13       345\n",
      "\n",
      "                                            accuracy                           0.76     17522\n",
      "                                           macro avg       0.49      0.35      0.38     17522\n",
      "                                        weighted avg       0.72      0.76      0.72     17522\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DistilBERT",
   "id": "3efaacfa993e6647"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import DistilBertTokenizer, DistilBertModel as DistilBert\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.data_parallel as dp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.utils.utils as xu\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import csv\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "data= pd.read_csv('/kaggle/input/data-file/cleaned_train.csv')\n",
    "tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "encodings=tokenizer.batch_encode_plus(\n",
    "    data['text'].astype(str).tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encodings.input_ids\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import DistilBertTokenizer, \n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "\n",
    "\n",
    "class TextDataset():\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Encode the label using label_encoder\n",
    "        label = self.label_encoder.transform([self.labels[idx]])[0]\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 128 \n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = TextDataset(data['text'].tolist(), data['sub_category'].tolist(), tokenizer, max_len)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data['sub_category'])\n",
    "\n",
    "train_dataset.label_encoder = label_encoder\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Converting the pandas DataFrame to a Hugging Face Dataset\n",
    "data['text'] = data['text'].astype(str)\n",
    "train_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Initializing the label encoder and fitting it to the 'sub_category' column\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data['sub_category'])\n",
    "\n",
    "# Label encoding function\n",
    "def label_encode(examples):\n",
    "    examples['labels'] = label_encoder.transform(examples['sub_category'])\n",
    "    return examples\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Applying label encoding and preprocessing\n",
    "train_dataset = train_dataset.map(label_encode, batched=True)\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "train_dataset = train_dataset.remove_columns(['sub_category'])\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Load model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# TPU device\n",
    "device = xm.xla_device()\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Wrap DataLoader for TPU parallelization\n",
    "    para_loader = pl.ParallelLoader(train_dataloader, [device])\n",
    "    train_device_loader = para_loader.per_device_loader(device)\n",
    "    \n",
    "    progress_bar = tqdm(train_device_loader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        xm.optimizer_step(optimizer) \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(train_device_loader)}\")\n",
    "    \n",
    "    # Save model after each epoch\n",
    "    xm.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=39)\n",
    "\n",
    "# Load the weights from the .pth file\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/distil/transformers/default/1/Model Epoch 2.pth\", map_location=\"cpu\"))\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"model_dir\")\n",
    "\n",
    "# Save the tokenizer (use the same tokenizer as during training)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer.save_pretrained(\"model_dir\")\n",
    "\n",
    "\n",
    "# Set the TPU device\n",
    "device = xm.xla_device()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"/kaggle/working/model_dir\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"/kaggle/working/model_dir\")\n",
    "\n",
    "# Move model to TPU\n",
    "model.to(device)\n",
    "\n",
    "# Function to load the category map from CSV\n",
    "def create_category_map(csv_file_path):\n",
    "    category_map = {}\n",
    "    with open(csv_file_path, mode='r', encoding='latin-1') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            category = row['category']\n",
    "            sub_category = row['sub_category']\n",
    "            category_map[sub_category] = category\n",
    "    return category_map\n",
    "\n",
    "# Function to retrieve the main category for a sub-category\n",
    "def get_category_by_sub_category(sub_category, category_map):\n",
    "    return category_map.get(sub_category, \"Sub-category not found\")\n",
    "\n",
    "# Prediction function\n",
    "def predict(text):\n",
    "    # Tokenize and move inputs to the TPU device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
    "    \n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = outputs.logits.softmax(dim=1)\n",
    "        predicted_class = label_encoder.inverse_transform([probs.argmax().item()])\n",
    "    return predicted_class\n",
    "\n",
    "# Example: Predict and get category\n",
    "text = \"\"\"Identity theft   Fake Customer Care Service Fraud   Face Book\n",
    "victim got call from suspect that he conntacted on facebook to victim whatsapp number while calling he catured his photo and make a nude vedio call and demanding money if not he will upload in his contacts so victim sent money   to suspect\"\"\"\n",
    "prediction = predict(text)\n",
    "print(\"Predicted sub-category:\", prediction[0])\n",
    "\n",
    "# Load the category map from your CSV file\n",
    "category_map = create_category_map('/kaggle/input/data-file/train.csv')\n",
    "\n",
    "# Find the main category for the predicted sub-category\n",
    "sub_category_to_lookup = prediction[0]\n",
    "category = get_category_by_sub_category(sub_category_to_lookup, category_map)\n",
    "print(f\"Category for sub-category '{sub_category_to_lookup}': {category}\")\n"
   ],
   "id": "a79c072faac45391"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
