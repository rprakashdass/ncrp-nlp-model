{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T14:59:59.429226Z",
     "start_time": "2024-11-22T14:44:01.583525Z"
    }
   },
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "data = pd.read_csv('cleaned_train.csv')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "max_words = 10000  # Limit the number of unique words to 10,000\n",
    "max_sequence_length = 100\n",
    "\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "X = tokenizer.texts_to_sequences(data['text'])\n",
    "X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "# Label Encoding the categories (You can also encode subcategories in a similar way if needed)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['category'])\n",
    "y = to_categorical(y)  # Convert labels to one-hot encoding\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Build the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))  # Output layer for multi-class classification\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "# Save the model\n",
    "model.save('lstm_model.h5')\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the Pre-Trained Model\n",
    "model = load_model('lstm_model.h5')  # Load your pre-trained model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 2. Load the Dataset (cleaned_test.csv)\n",
    "df = pd.read_csv(\"cleaned_test.csv\")\n",
    "\n",
    "# Ensure the 'text' column contains strings\n",
    "df['text'] = df['text'].astype(str).fillna(\"\")\n",
    "X_test_data = df['text']\n",
    "\n",
    "# Ensure the 'category' column has no NaN values\n",
    "df['category'] = df['category'].fillna(\"Unknown\")\n",
    "y_test = df['category']\n",
    "\n",
    "# 3. Tokenization and Padding\n",
    "# Initialize the Tokenizer (same parameters as during training)\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_test_data)  # Use the same tokenizer as during training\n",
    "\n",
    "# Tokenize and pad test data\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_data)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "# 4. Predict Using the Pre-Trained Model\n",
    "y_pred_prob = model.predict(X_test_padded)  # Get predicted probabilities\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# 5. Encode the True Labels (if LabelEncoder was used during training)\n",
    "label_encoder = LabelEncoder()\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)  # Encode the true labels\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"absl\")\n",
    "# 6. Calculate the Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "precision = precision_score(y_test_encoded, y_pred, average='weighted')\n",
    "recall = recall_score(y_test_encoded, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test_encoded, y_pred, average='weighted')\n",
    "\n",
    "# 7. Print the Metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pradeesh11/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pradeesh11/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m1096/1096\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m347s\u001B[0m 315ms/step - accuracy: 0.6912 - loss: 1.0392 - val_accuracy: 0.7426 - val_loss: 0.7685\n",
      "Epoch 2/5\n",
      "\u001B[1m1096/1096\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m256s\u001B[0m 234ms/step - accuracy: 0.7461 - loss: 0.7529 - val_accuracy: 0.7504 - val_loss: 0.7368\n",
      "Epoch 3/5\n",
      "\u001B[1m1096/1096\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m101s\u001B[0m 92ms/step - accuracy: 0.7641 - loss: 0.6831 - val_accuracy: 0.7460 - val_loss: 0.7240\n",
      "Epoch 4/5\n",
      "\u001B[1m1096/1096\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m106s\u001B[0m 97ms/step - accuracy: 0.7819 - loss: 0.6307 - val_accuracy: 0.7547 - val_loss: 0.7288\n",
      "Epoch 5/5\n",
      "\u001B[1m1096/1096\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m106s\u001B[0m 96ms/step - accuracy: 0.8061 - loss: 0.5631 - val_accuracy: 0.7491 - val_loss: 0.7808\n",
      "\u001B[1m548/548\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 17ms/step - accuracy: 0.7437 - loss: 0.7988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 74.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m976/976\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 15ms/step\n",
      "Accuracy: 0.0035\n",
      "Precision: 0.0284\n",
      "Recall: 0.0035\n",
      "F1 Score: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradeesh11/Documents/ncrp-nlp-model/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b68946374a5c34b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
