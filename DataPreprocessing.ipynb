{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocessing the dataset",
   "id": "5e666dee575729da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ],
   "id": "e23dac7f3dc70afe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"train.csv\", encoding=\"ISO-8859-1\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "data = data.rename(columns={'crimeaditionalinfo': 'text'})\n",
    "data = data.dropna(subset=['text'])\n",
    "test = test.rename(columns={'crimeaditionalinfo': 'text'}) # renaming the crime info column\n",
    "test = test.dropna(subset=['text']) # dropping all entries with no information on the crime"
   ],
   "id": "ed73f2c8624cfeb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "null_count = data['sub_category'].isnull().sum()\n",
    "data['sub_category'].fillna(data['category'], inplace=True)\n",
    "test['sub_category'].fillna(data['category'], inplace=True) # replacing the null entries in sub_category column with the category of the complaint\n",
    "print('null count: ', null_count)\n",
    "data.groupby('category')['sub_category'].value_counts() # number of entries and sub categories under each categories"
   ],
   "id": "edf05ea6b2334974"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mapping = data.set_index('sub_category')['category'].to_dict()\n",
    "def get_category(sub_category):\n",
    "    return mapping.get(sub_category)\n",
    "subcategories = data['sub_category'].unique().tolist()\n"
   ],
   "id": "a26b406754b2feef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unique_data_subcategories = set(data['sub_category'].unique())\n",
    "unique_test_subcategories = set(test['sub_category'].unique())\n",
    "\n",
    "exclusive_in_data_subcategories = unique_data_subcategories - unique_test_subcategories\n",
    "exclusive_in_test_subcategories = unique_test_subcategories - unique_data_subcategories\n",
    "\n",
    "print(\"Subcategories exclusive to 'train' dataframe:\")\n",
    "print(list(exclusive_in_data_subcategories))\n",
    "print(\"\\nSubcategories exclusive to 'test' dataframe:\")\n",
    "print(list(exclusive_in_test_subcategories))\n",
    "exclusive_in_test_subcategories"
   ],
   "id": "b63baa96b54f6b3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unique_data_categories = set(data['category'].unique())\n",
    "unique_test_categories = set(test['category'].unique())\n",
    "\n",
    "exclusive_in_data = unique_data_categories - unique_test_categories\n",
    "exclusive_in_test = unique_test_categories - unique_data_categories\n",
    "\n",
    "exclusive_in_data, exclusive_in_test\n"
   ],
   "id": "2554d43d83d74d0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = data[data['word_count'] >= 4]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['text'] = data['text'].str.lower().apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
    "test['text'] = test['text'].str.lower().apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))"
   ],
   "id": "e06d2bac4d00b092"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Removing all instances which are not of type string\n",
    "data = data[data['text'].apply(lambda x: isinstance(x, str))]\n",
    "test = test[test['text'].apply(lambda x: isinstance(x, str))]\n",
    "total_char_count = data['text'].str.len().sum()\n",
    "total_char_count"
   ],
   "id": "dd1268e5fa11e561"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def Text_Cleaning(Text):\n",
    "  Text = Text.lower()\n",
    "  punc = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "  Text = Text.translate(punc)\n",
    "  Text = re.sub(r'\\d+', '', Text)\n",
    "  Text = re.sub('https?://\\S+|www\\.\\S+', '', Text)\n",
    "  Text = re.sub('\\n', '', Text)\n",
    "  return Text\n",
    "Stopwords = set(nltk.corpus.stopwords.words(\"english\")) - set([\"not\"])\n",
    "\n",
    "def Text_Processing(Text):\n",
    "    Processed_Text = list()\n",
    "    Lemmatizer = WordNetLemmatizer()\n",
    "    Tokens = nltk.word_tokenize(Text)\n",
    "\n",
    "    for word in Tokens:\n",
    "        if word not in Stopwords:\n",
    "            Processed_Text.append(Lemmatizer.lemmatize(word))\n",
    "\n",
    "    return \" \".join(Processed_Text)"
   ],
   "id": "334a586fc28fc19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data[\"text\"] = data[\"text\"].apply(lambda Text: Text_Cleaning(Text))\n",
    "test[\"text\"] = test[\"text\"].apply(lambda Text: Text_Cleaning(Text))\n",
    "data[\"text\"] = data[\"text\"].apply(lambda Text: Text_Processing(Text))\n",
    "test[\"text\"] = test[\"text\"].apply(lambda Text: Text_Processing(Text))\n",
    "total_char_count = data['text'].str.len().sum()\n",
    "total_char_count"
   ],
   "id": "2816f4cfebf22202"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stemmer = PorterStemmer()\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(x)]))\n",
    "test['text'] = test['text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(x)]))"
   ],
   "id": "97081dfb0a411846"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data.to_csv('cleaned_train.csv', index=False)\n",
    "test.to_csv('cleaned_test.csv', index=False)"
   ],
   "id": "d3f1778d15f311aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
